{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e84e0e2-dc5c-4ef2-b6d5-5c0bcc241a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#決定要不要放入影片、照片、連結的文章，回傳Ｔ/Ｆ\n",
    "def putinOrNot(s):\n",
    "    dele = ['網傳影','網傳照片','網傳圖','網傳連結']\n",
    "    for i in dele:\n",
    "        if i in s:\n",
    "            return False\n",
    "    else: return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3206136-1291-403d-9f9e-bd0b40d9ae3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#抓取該post的label\n",
    "def Tag(s):\n",
    "    try:\n",
    "        tag=[]\n",
    "        lBracket = s.index('【')\n",
    "        rBracket = s.index('】')\n",
    "        tag = s[lBracket+1:rBracket]\n",
    "        return tag,rBracket\n",
    "    except:\n",
    "        return 'unknown',-1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bbec462-c21b-40ef-ad0d-7df4576b7bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#回傳今天的日期：yyyymmdd\n",
    "def Today():\n",
    "    import datetime\n",
    "    date = str(datetime.date.today()).replace(\"-\",\"\")\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc36ba82-092e-47a9-9b29-2bf3e9d11003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#回傳最後一篇的page編號\n",
    "def LastestPage():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    \n",
    "    url = \"https://tfc-taiwan.org.tw\"\n",
    "    r = requests.get(url)\n",
    "    r.encoding ='utf-8'\n",
    "    soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "    last = soup.select_one(\"h3.entity-list-title a\")\n",
    "    num = int(last['href'][10:])\n",
    "    return num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96ccf2a0-a59c-489d-aa67-d4bc7864dac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n"
     ]
    }
   ],
   "source": [
    "#tfc只爬標題 json版\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "date = str(Today())\n",
    "OUTPUT_FILE = '/Users/dayuan/Desktop/爬蟲/Output/'+date+'_tfc.json'\n",
    "\n",
    "results=[]\n",
    "for p in range(10,228):\n",
    "    if p == 0:\n",
    "        link =\"https://tfc-taiwan.org.tw/articles/report\"\n",
    "    else:\n",
    "        link = \"https://tfc-taiwan.org.tw/articles/report?page=\"+str(p)\n",
    "\n",
    "    r = requests.get(link)\n",
    "    r.encoding ='utf-8'\n",
    "    soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "    \n",
    "    news = soup.select(\"div.right-pane.views-fieldset\")\n",
    "    for n in news:\n",
    "        \n",
    "        #title and label        \n",
    "        title = n.find(\"h3\",class_=\"entity-list-title\").text\n",
    "        tag,start = Tag(title)\n",
    "        label = tag\n",
    "        title = title[start+1:]\n",
    "        \n",
    "        #url\n",
    "        href = n.find(class_=\"entity-list-title\").find(\"a\")['href']\n",
    "        url = \"https://tfc-taiwan.org.tw/\"+href\n",
    "        \n",
    "        #date\n",
    "        d = n.find(\"div\",class_= \"post-date\").text\n",
    "        d = d.replace('發布日期：','')\n",
    "        post_date = d\n",
    "         \n",
    "        #domain    \n",
    "        domain = n.find(\"div\",class_=\"attr-tag\").text\n",
    "        \n",
    "        #content\n",
    "        #content = n.find(\"div\",class_=\"entity-list-body\").text \n",
    "        #【去除報告將隨時更新 yyyy/mm/dd版 \\n\\n】\n",
    "        #t,start = Tag(content) \n",
    "        #content = content[start+3:]\n",
    "\n",
    "\n",
    "        result ={'label':label,\n",
    "                 'title':title,\n",
    "                 'time':post_date,\n",
    "                 'domain':domain,\n",
    "                 #'content':content,\n",
    "                 'url':url\n",
    "                }\n",
    "\n",
    "        results.append(result)\n",
    "    with open(OUTPUT_FILE,'w',newline='',encoding='utf-8')as ofile:\n",
    "        json.dump(results, ofile, indent=2, ensure_ascii=False)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342ceb3a-120a-46c4-9d08-00f2642bc51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "#tfc只爬標題 csv版\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "date = str(Today())\n",
    "OUTPUT_FILE = '/Users/dayuan/Desktop/爬蟲/Output/'+date+'_tfc.csv'\n",
    "\n",
    "with open(OUTPUT_FILE,'w',newline='',encoding='utf-8')as ofile:\n",
    "    writer = csv.writer(ofile)\n",
    "    #writer.writerow(['label','post_date','domain','title','context','link'])\n",
    "    writer.writerow(['label','post_date','domain','title','link'])\n",
    "\n",
    "    for p in range(10,227):\n",
    "        href=[]\n",
    "        title=[]\n",
    "        post_date=[]\n",
    "        domain=[]\n",
    "        #context=[]\n",
    "       \n",
    "        if p == 0:\n",
    "            link =\"https://tfc-taiwan.org.tw/articles/report\"\n",
    "        else:\n",
    "            link = \"https://tfc-taiwan.org.tw/articles/report?page=\"+str(p)\n",
    "        \n",
    "        r = requests.get(link)\n",
    "        r.encoding ='utf-8'\n",
    "        soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "        \n",
    "        sel = soup.select(\"h3.entity-list-title a\")\n",
    "        for s in sel:\n",
    "            href.append(\"https://tfc-taiwan.org.tw/\"+s[\"href\"])\n",
    "            title.append(s.text)\n",
    "        \n",
    "        selD = soup.select(\"div.post-date\")\n",
    "        for d in selD:\n",
    "            d = d.text\n",
    "            d = d.replace('發布日期：','')\n",
    "            post_date.append(d)\n",
    "\n",
    "        selT = soup.select(\"div.attr-tag\")\n",
    "        for t in selT:\n",
    "            domain.append(t.text)\n",
    "\n",
    "        '''\n",
    "        selB = soup.select(\"div.entity-list-body\")\n",
    "        for b in selB:\n",
    "            context.append(b.text)\n",
    "        '''\n",
    "        \n",
    "        for i in range(len(href)):\n",
    "            #put = putinOrNot(title[i])\n",
    "            #if put == True:\n",
    "            tag,start = Tag(title[i])\n",
    "            #writer.writerow([tag,post_date[i],domain[i],title[i][start+1:],context[i],href[i]])\n",
    "            writer.writerow([tag,post_date[i],domain[i],title[i][start+1:],href[i]])\n",
    "            #print(put,tag,post_date[i],domain[i],title[i][start+1:],href[i])\n",
    "        print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a24688-2b45-4071-bbf2-aece555dd25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfc想爬全部內容\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "OUTPUT_FILE = '/Users/dayuan/Desktop/爬蟲/'+Today()+'_tfc.csv'\n",
    "\n",
    "num = LastestPage()\n",
    "\n",
    "\n",
    "with open(OUTPUT_FILE,'w',newline='',encoding='utf-8')as ofile:\n",
    "    writer = csv.writer(ofile)\n",
    "    writer.writerow(['label','post_date','domain','title','link'])\n",
    "\n",
    "    for i in range(num,73,-1):\n",
    "        url = \"https://tfc-taiwan.org.tw/articles/\"+str(i)\n",
    "        #post = {\"label\":\"NULL\",\"post_date\":\"NULL\",\"domain\":\"NULL\",\"title\":\"NULL\",\"link\":\"NULL\",\"content\":\"NULL\"}\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            r.encoding ='utf-8'\n",
    "            soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "            print(i)\n",
    "        except:\n",
    "            print(\"not find\",url)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        sel = soup.select(\"h3.entity-list-title a\")\n",
    "        for s in sel:\n",
    "            href.append(\"https://tfc-taiwan.org.tw/\"+s[\"href\"])\n",
    "            title.append(s.text)\n",
    "        \n",
    "        selD = soup.select(\"div.post-date\")\n",
    "        for d in selD:\n",
    "            d = d.text\n",
    "            d = d.replace('發布日期：','')\n",
    "            post_date.append(d)\n",
    "\n",
    "        selT = soup.select(\"div.attr-tag\")\n",
    "        for t in selT:\n",
    "            domain.append(t.text)\n",
    "\n",
    "        selB = soup.select(\"div.entity-list-body\")\n",
    "        #for b in selB:\n",
    "            #context.append(b.text)\n",
    "\n",
    "        for i in range(len(href)):\n",
    "            put = putinOrNot(title[i])\n",
    "            if put == True:\n",
    "                tag,start = Tag(title[i])\n",
    "                writer.writerow([tag,post_date[i],domain[i],title[i][start+1:],href[i]])\n",
    "                #print(put,tag,post_date[i],domain[i],title[i][start+1:],href[i])\n",
    "        print(p)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e32f1d-8044-4e3d-9048-8a58802da345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "INPUT_FILE = '/Users/dayuan/Desktop/爬蟲/1120208_tfc.csv'\n",
    "\n",
    "with open(INPUT_FILE,'r',newline='',encoding='utf-8')as ifile:\n",
    "    rows = csv.reader(ifile)\n",
    "    ifile.readline() #讀掉標題\n",
    "    for r in rows:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f1d78-e113-4cce-bcc3-4722ca0da454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#看post\n",
    "import json\n",
    "\n",
    "with open('/Users/dayuan/Desktop/News-Environment-Perception/dataset/Chinese/post/train.json','r') as ifile:\n",
    "    post_dict = json.load(ifile)\n",
    "    print(post_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c79c90-47ca-4c77-85f8-711878ad8c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
